{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEb2BfcuFggz"
   },
   "source": [
    "# Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rs6HRbhWFgg0",
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WZW-pm6Fgg0"
   },
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS0vdd5mFgg0"
   },
   "source": [
    "Carga los datos de texto del archivo 'imdb_reviews_small.tsv' file.\n",
    "\n",
    "Es un archivo de valores separados por tabuladores (TSV), lo cual significa que cada uno de los campos están separados por tabuladores (en lugar de comas, como has visto en otros ejercicios de TripleTen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6nimxZzEFgg0"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/datasets/imdb_reviews_small.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0emU20IFgg0"
   },
   "source": [
    "# Tokenizador BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_i0JMPAFgg1"
   },
   "source": [
    "Crea el tokenizador BERT a partir de un modelo previamente entrenado que se llama 'bert-base-uncased' en transformadores. Puedes consultar rápidamente una descripción general [aquí](https://huggingface.co/transformers/pretrained_models.html), Y para obtener más detalles, puedes leer [aquí](https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ObHWqouUFgg1",
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEoVM_tbFgg1"
   },
   "source": [
    "Hay un ejemplo de cómo obtener tokens para un solo texto dado.\n",
    "\n",
    "Puedes usarlo para procesar todos los datos que cargaste anteriormente. Como ya hay muchos textos, y es probable que los proceses en un bucle, las longitudes mínimas/máximas de los vectores se pueden calcular de dos formas: dentro de un bucle o después de un bucle.\n",
    "\n",
    "En el último caso, los vectores de identificadores numéricos de tokens (`ids`) y máscaras de atención (`attention_mask`) se deben almacenar en dos listas separadas. Se pueden llamar `ids_list` y `atencion_mask_list`, respectivamente. El primer caso te permite evitar la creación de esas listas, a menos que desees utilizarlas con otra finalidad, por ejemplo, para propagarlas en un modelo BERT. No se requiere en esta tarea, pero se requerirá en el proyecto.\n",
    "\n",
    "Teniendo en cuenta lo anterior, es posible que desees combinar ambas formas para calcular las longitudes mínimas/máximas de los vectores para tokens y máscaras de atención, y conservar el resultado del tokenizador para su posterior procesamiento. Solo considera que no tiene mucho sentido mantener vectores de más de 512 elementos, ya que esta es la longitud máxima de vectores que BERT puede aceptar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ULoht5CBFgg1",
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "outputs": [],
   "source": [
    "# textos a tokens\n",
    "text = 'Es muy práctico utilizar transformadores'\n",
    "\n",
    "# añadiendo este truco para suprimir avisos de salidas demasiado largas\n",
    "# normalmente no necesitamos eso, pero en este caso queremos explorar\n",
    "# cuál es la longitud máxima de ID para nuestro conjunto de reseñas\n",
    "# por eso no truncamos la salida (ids) de max_length\n",
    "# con los parámetros max_length=max_length y truncation=True\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "ids = tokenizer.encode(text.lower(), add_special_tokens=True)\n",
    "\n",
    "# padding (agregar ceros al vector para hacer que su longitud sea igual a n)\n",
    "n = 512\n",
    "padded = np.array(ids[:n] + [0]*(n - len(ids)))\n",
    "\n",
    "# creación de la máscara de atención para distinguir los tokens que nos interesan\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OC_yLLkgFgg1",
    "outputId": "bf51c606-70fe-4d80-8db3-6858932abf0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9686, 14163, 2100, 10975, 28804, 2080, 21183, 18622, 9057, 10938, 26467, 2229, 102]\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xWexyCiAFgg1",
    "outputId": "dd0ce914-79b9-4660-a27a-1f526d88a5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  9686 14163  2100 10975 28804  2080 21183 18622  9057 10938 26467\n",
      "  2229   102     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ksB1bKGLFgg1",
    "outputId": "8a15b602-03b3-4592-d0d3-514e2494cb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD4l29zOFgg2"
   },
   "source": [
    "Escribe tu código para tokenizar los datos de texto cargados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gNS4vN2FFgg2"
   },
   "outputs": [],
   "source": [
    "def tokenize_with_bert(texts, max_length=512):\n",
    "    ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    min_tokenized_text_length = 1e7\n",
    "    max_tokenized_text_length = 0\n",
    "\n",
    "    for text in texts:\n",
    "\n",
    "        # Suprimir advertencias sobre salidas largas\n",
    "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "        # Tokenizar el texto en minúsculas, agregar tokens especiales\n",
    "        ids = tokenizer.encode(text.lower(), add_special_tokens=True)\n",
    "\n",
    "        ids_len = len(ids)\n",
    "\n",
    "        # Actualizar longitudes mínimas y máximas\n",
    "        if ids_len < min_tokenized_text_length:\n",
    "            min_tokenized_text_length = ids_len\n",
    "        if ids_len > max_tokenized_text_length:\n",
    "            max_tokenized_text_length = ids_len\n",
    "\n",
    "        # Truncar si excede el máximo permitido por BERT\n",
    "        if ids_len > max_length:\n",
    "            ids = ids[:max_length]\n",
    "\n",
    "        # Rellenar hasta max_length con ceros (padding token ID = 0 en BERT)\n",
    "        padded = ids + [0] * (max_length - len(ids))\n",
    "\n",
    "        # Crear máscara de atención: 1 para tokens reales, 0 para padding\n",
    "        attention_mask = [1] * len(ids) + [0] * (max_length - len(ids))\n",
    "\n",
    "        ids_list.append(padded)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "\n",
    "    print(f'La longitud mínima de los vectores: {min_tokenized_text_length}')\n",
    "    print(f'La longitud máxima de los vectores: {max_tokenized_text_length}')\n",
    "\n",
    "    return ids_list, attention_mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP1pTBfCFgg2"
   },
   "source": [
    "Ejecuta el tokenizador para todos los datos. Puede tomar un poco de tiempo como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rv0oGP0lFgg2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud mínima de los vectores: 18\n",
      "La longitud máxima de los vectores: 3047\n"
     ]
    }
   ],
   "source": [
    "ids_list, attention_mask_list = tokenize_with_bert(texts=data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmwbyHksJ-en"
   },
   "source": [
    "Veamos algunos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Pb3xSjLyFgg2"
   },
   "outputs": [],
   "source": [
    "# cuenta elementos distintos de cero\n",
    "ids_array = np.count_nonzero(np.array(ids_list), axis=1)\n",
    "# almacenar el índice de la revisión que tiene la menor cantidad de tokens después de la tokenización\n",
    "short_review_idx = np.argmin(np.count_nonzero(np.array(ids_list), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wvcfQJ5GLC3K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'More suspenseful, more subtle, much, much more disturbing....'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la reseña más corta\n",
    "data['review'].iloc[short_review_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LGARhpgKOTSW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2062,\n",
       " 23873,\n",
       " 3993,\n",
       " 1010,\n",
       " 2062,\n",
       " 11259,\n",
       " 1010,\n",
       " 2172,\n",
       " 1010,\n",
       " 2172,\n",
       " 2062,\n",
       " 14888,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorporación de la reseña más corta\n",
    "ids_list[short_review_idx][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2eZacqBIOT2w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] more suspenseful , more subtle , much , much more disturbing . . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisión más breve integrada basada en BERT\n",
    "tokenizer.decode(ids_list[short_review_idx][:50], clean_up_tokenization_spaces=False)"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 927,
    "start_time": "2025-06-16T23:39:32.976Z"
   },
   {
    "duration": 568,
    "start_time": "2025-06-16T23:40:05.595Z"
   },
   {
    "duration": 114,
    "start_time": "2025-06-16T23:40:28.799Z"
   },
   {
    "duration": 57,
    "start_time": "2025-06-16T23:41:50.501Z"
   },
   {
    "duration": 1610,
    "start_time": "2025-06-16T23:41:57.922Z"
   },
   {
    "duration": 4,
    "start_time": "2025-06-16T23:43:05.885Z"
   },
   {
    "duration": 2,
    "start_time": "2025-06-16T23:43:09.861Z"
   },
   {
    "duration": 3,
    "start_time": "2025-06-16T23:43:11.680Z"
   },
   {
    "duration": 3,
    "start_time": "2025-06-16T23:43:14.748Z"
   },
   {
    "duration": 6,
    "start_time": "2025-06-16T23:43:17.975Z"
   },
   {
    "duration": 199,
    "start_time": "2025-06-16T23:43:21.370Z"
   },
   {
    "duration": 5,
    "start_time": "2025-06-16T23:45:26.313Z"
   },
   {
    "duration": 12692,
    "start_time": "2025-06-16T23:45:50.465Z"
   },
   {
    "duration": 209,
    "start_time": "2025-06-16T23:46:17.266Z"
   },
   {
    "duration": 5,
    "start_time": "2025-06-16T23:46:20.841Z"
   },
   {
    "duration": 4,
    "start_time": "2025-06-16T23:46:26.958Z"
   },
   {
    "duration": 4,
    "start_time": "2025-06-16T23:46:31.957Z"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
